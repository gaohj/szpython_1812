# Settings配置文件

## 常用的配置项：

1. `BOT_NAME`：项目名称。

2. `ROBOTSTXT_OBEY`：是否遵守爬虫协议。默认不遵守。

3. `CONCURRENT_ITEMS`：代表`pipeline`同时处理的`item`数的最大值。默认是100

4. `CONCURRENT_REQUESTS`：代表下载器并发请求的最大是，默认是16。

5. `DEFAULT_REQUEST_HEADERS`：默认请求头。可以将一些不会经常变化的请求头放在这个里面。

6. `DEPTH_LIMIT`：爬取网站最大允许的深度。默认为0，如果为0，则没有限制。

7. `DOWNLOAD_DELAY`：下载器在下载某个页面前等待多长的时间。该选项用来限制爬虫的爬取速度，减轻服务器压力。同时也支持小数。

8. `DOWNLOAD_TIMEOUT`：下载器下载的超时时间。

9. `ITEM_PIPELINES`：处理`item`的`Pipeline`，是一个字典，字典的key这个pipeline所在包的绝对路径，值是一个整数，优先级，值越小，优先级越高。

10. `LOG_ENABLED`：是否启用logging。默认是True。

11. `LOG_ENCODING`：log的编码。

12. `LOG_LEVEL`：log的级别。默认为`DEBUG`。可选的级别有`CRITICAL`、`ERROR`、`WARNING`、`INFO`、`DEBUG`。

13. `USER_AGENT`：请求头。默认为`Scrapy/VERSION (+http://scrapy.org)`。

14. `PROXIES`：代理设置。

15. `COOKIES_ENABLED`：是否开启cookie。一般不要开启，避免爬虫被追踪到。如果特殊情况也可以开启。


# 下载文件和图片

Scrapy为下载item中包含的文件(比如在爬取到产品时，同时也想保存对应的图片)提供了一个可重用的`item pipelines`。这些`pipeline`有些共同的方法和结构(我们称之为`media pipeline`)。一般来说你会使用`Files Pipeline`或者`Images Pipeline`。

## 为什么要选择使用`scrapy`内置的下载文件的方法：

1. 避免重新下载最近已经下载过的文件。
2. 可以方便的指定文件存储的路径。
3. 可以将下载的图片转换成通用的格式。比如png或jpg。
4. 可以方便的生成缩略图。
5. 可以方便的检测图片的宽和高，确保他们满足最小限制。
6. 异步下载，效率非常高。

## 下载文件的`Files Pipeline`：

当使用`Files Pipeline`下载文件的时候，按照以下步骤来完成：

1. 定义好一个`Item`，然后在这个`item`中定义两个属性，分别为`file_urls`以及`files`。`file_urls`是用来存储需要下载的文件的url链接，需要给一个列表。
2. 当文件下载完成后，会把文件下载的相关信息存储到`item`的`files`属性中。比如下载路径、下载的url和文件的校验码等。
3. 在配置文件`settings.py`中配置`FILES_STORE`，这个配置是用来设置文件下载下来的路径。
4. 启动`pipeline`：在`ITEM_PIPELINES`中设置`scrapy.pipelines.files.FilesPipeline:1`。

## 下载图片的`Images Pipeline`：

当使用`Images Pipeline`下载文件的时候，按照以下步骤来完成：

1. 定义好一个`Item`，然后在这个`item`中定义两个属性，分别为`image_urls`以及`images`。`image_urls`是用来存储需要下载的图片的url链接，需要给一个列表。
2. 当文件下载完成后，会把文件下载的相关信息存储到`item`的`images`属性中。比如下载路径、下载的url和图片的校验码等。
3. 在配置文件`settings.py`中配置`IMAGES_STORE`，这个配置是用来设置图片下载下来的路径。
4. 启动`pipeline`：在`ITEM_PIPELINES`中设置`scrapy.pipelines.images.ImagesPipeline:1`。

<<<<<<< HEAD
<<<<<<< HEAD
> 但是有个问题 所有的图片都在 images/full下面 如果想要自定义目录 需要我们在 piplines 中自定义类 
>
> 要继承于  ImagesPipeline  （点进去看详情 ）
>
> ImagesPipeline   中的file_path 方法 决定了图片存在哪个地方  图片存储路径 = 分类目录 +图片名
>
> 需要我们重写 file_path   （  这个方法是在图片将要保存的时候调用 获取图片存储的路径）
>
> 但是我们获取不了图片所在的目录  我们只能在发送请求之前做文章 拿下来   
>
> 这时候我们需要查看 get_media_requests 层层向上 发现 能追踪到 start_urls  继承于父类的方法 返回requests  我们将我们yiled 收集的item 通过这个 requests 对象 传给 file_path   
>
> file_path 的request 里边包含我们 收集的item  item中有分类  就可以获取到了   
>
> 



```
class BMWImagesPipline(ImagesPipeline):
    def get_media_requests(self, item, info):
        #这个方法是在发送下载请求之前调用
        #也就是说这个方法 就是去发送下载请求
        requests_objs =super(BMWImagesPipline, self).get_media_requests(item,info)
        for requests_obj in requests_objs:
            requests_obj.item = item
        return requests_objs
    def file_path(self, request, response=None, info=None):
        #这个方法是在图片将要保存的时候调用 获取图片存储的路径
        path = super(BMWImagesPipline, self).file_path(request,response,info)
        #path返回的就是图片的路径
        #但是返回的是 full/asdfads.jpg
        category = request.item.get('category')
        #file_path仅仅是获取图片存储的路径 分类不能拿到  需要在发送请求之前拿到分类
        #需要重写get_media_requests 方法
        image_store = settings.IMAGES_STORE
        category_path = os.path.join(image_store,category)
        if not os.path.exists(category_path):
            os.mkdir(category_path)
        image_name = path.replace("full/","") #图片的名字
        image_path = os.path.join(category_path,image_name)
        return image_path

```


# Downloader Middlewares（下载器中间件）

下载器中间件是引擎和下载器之间通信的中间件。在这个中间件中我们可以设置代理、更换请求头等来达到反反爬虫的目的。要写下载器中间件，可以在下载器中实现两个方法。一个是`process_request(self,request,spider)`，这个方法是在请求发送之前会执行，还有一个是`process_response(self,request,response,spider)`，这个方法是数据下载到引擎之前执行。

## process_request(self,request,spider)：

这个方法是下载器在发送请求之前会执行的。一般可以在这个里面设置随机代理ip等。

1. 参数：
   - request：发送请求的request对象。
   - spider：发送请求的spider对象。
2. 返回值：
   - 返回None：如果返回None，Scrapy将继续处理该request，执行其他中间件中的相应方法，直到合适的下载器处理函数被调用。
   - 返回Response对象：Scrapy将不会调用任何其他的`process_request`方法，将直接返回这个response对象。已经激活的中间件的process_response()方法则会在每个response返回时被调用。
   - 返回Request对象：不再使用之前的request对象去下载数据，而是根据现在返回的request对象返回数据。
   - 如果这个方法中抛出了异常，则会调用process_exception方法。

## process_response(self,request,response,spider)：

这个是下载器下载的数据到引擎中间会执行的方法。

1. 参数：
   - request：request对象。
   - response：被处理的response对象。
   - spider：spider对象。
2. 返回值：
   - 返回Response对象：会将这个新的response对象传给其他中间件，最终传给爬虫。
   - 返回Request对象：下载器链被切断，返回的request会重新被下载器调度下载。
   - 如果抛出一个异常，那么调用request的`errback`方法，如果没有指定这个方法，那么会抛出一个异常。

## 随机请求头中间件：

爬虫在频繁访问一个页面的时候，这个请求头如果一直保持一致。那么很容易被服务器发现，从而禁止掉这个请求头的访问。因此我们要在访问这个页面之前随机的更改请求头，这样才可以避免爬虫被抓。随机更改请求头，可以在下载中间件中实现。在请求发送给服务器之前，随机的选择一个请求头。这样就可以避免总使用一个请求头了。示例代码如下：

```
class UserAgentDownloadMiddleware(object):
    # user-agent随机请求头中间件
    USER_AGENTS = [
        'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36',
        'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Trident/5.0;',
        'Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0)',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.6; rv,2.0.1) Gecko/20100101 Firefox/4.0.1',
        'Mozilla/5.0 (Windows NT 6.1; rv,2.0.1) Gecko/20100101 Firefox/4.0.1'
    ]
    def process_request(self,request,spider):
        user_agent = random.choice(self.USER_AGENTS)
        request.headers['User-Agent'] = user_agent
```

user-agent列表：<http://www.useragentstring.com/pages/useragentstring.php?typ=Browser>

## ip代理池中间件

### 购买代理：

在以下代理商中购买代理：

1. 芝麻代理：<http://http.zhimaruanjian.com/>
2. 太阳代理：<http://http.taiyangruanjian.com/>
3. 快代理：<http://www.kuaidaili.com/>
4. 讯代理：<http://www.xdaili.cn/>
5. 蚂蚁代理：<http://www.mayidaili.com/>
   等购买代理。
<<<<<<< HEAD
<<<<<<< HEAD
6. 西瓜代理
=======
>>>>>>> c7bd8bff104fbf41cb4953006ae4266c6b6df688
=======
>>>>>>> c7bd8bff104fbf41cb4953006ae4266c6b6df688

### 使用ip代理池：

1. 开放代理池设置：

   ```
    class IPProxyDownloadMiddleware(object):
        PROXIES = [
            "5.196.189.50:8080",
            "134.17.141.44:8080",
            "178.49.136.84:8080",
            "45.55.132.29:82",
            "178.44.185.15:8080"
        ]
        def process_request(self,request,spider):
            # proxy = random.choice(self.PROXIES)
            # print('被选中的代理：%s' % proxy)
            # request.meta['proxy'] = "http://" + proxy
            proxy = "121.199.6.124:16816"
   <<<<<<< HEAD
   <<<<<<< HEAD
            user_password = "你的用户名:你的密码"
   =======
            user_password = "970138074:rcdj35ur"
   ```
>>>>>>> c7bd8bff104fbf41cb4953006ae4266c6b6df688
>>>>>>> =======
>>>>>>>             user_password = "970138074:rcdj35ur"
>>>>>>> c7bd8bff104fbf41cb4953006ae4266c6b6df688
>>>>>>>             request.meta['proxy'] = proxy
>>>>>>>             request.headers['Proxy-Authorization'] = 'Basic ' + base64.b64encode(user_password.encode('utf-8')).decode('utf-8')
>>>>>>>
>>>>>>>    ```
>>>>>>> 
>>>>>>>    ```

2. 独享代理池设置：

   ```
    class IPProxyDownloadMiddleware(object):
        def process_request(self,request,spider):
            proxy = '121.199.6.124:16816'
   <<<<<<< HEAD
   <<<<<<< HEAD
            user_password = "你的用户名:你的密码"
   =======
            user_password = "970138074:rcdj35ur"
   ```
>>>>>>> c7bd8bff104fbf41cb4953006ae4266c6b6df688
>>>>>>> =======
>>>>>>>             user_password = "970138074:rcdj35ur"
>>>>>>> c7bd8bff104fbf41cb4953006ae4266c6b6df688
>>>>>>>             request.meta['proxy'] = proxy
>>>>>>>             # bytes
>>>>>>>             b64_user_password = base64.b64encode(user_password.encode('utf-8'))
>>>>>>>             request.headers['Proxy-Authorization'] = 'Basic ' + b64_user_password.decode('utf-8')
>>>>>>> <<<<<<< HEAD
>>>>>>> <<<<<<< HEAD
>>>>>>>
>>>>>>>    ```
>>>>>>> 
>>>>>>>    ```

```
https://car2.autoimg.cn/cardfs/product/g30/M08/42/54/autohomecar__ChsEoFuqOkGAFZ1EAAikBJ_JgBI973.jpg#pvareaid=2042293

https://car2.autoimg.cn/cardfs/product/g30/M08/42/54/t_autohomecar__ChsEoFuqOkGAFZ1EAAikBJ_JgBI973.jpg
```

=======
   ```
>>>>>>> c7bd8bff104fbf41cb4953006ae4266c6b6df688
=======
   ```
>>>>>>> c7bd8bff104fbf41cb4953006ae4266c6b6df688
